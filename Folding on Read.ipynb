{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"2764d134-6512-4ab9-b32c-7d661abfb810\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      document.getElementById(\"2764d134-6512-4ab9-b32c-7d661abfb810\").textContent = \"BokehJS successfully loaded.\";\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"2764d134-6512-4ab9-b32c-7d661abfb810\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '2764d134-6512-4ab9-b32c-7d661abfb810' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      document.getElementById(\"2764d134-6512-4ab9-b32c-7d661abfb810\").textContent = \"BokehJS is loading...\";\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"2764d134-6512-4ab9-b32c-7d661abfb810\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "import homer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.bag as db\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.diagnostics import Profiler, ResourceProfiler, CacheProfiler\n",
    "import json\n",
    "from dask.diagnostics import visualize\n",
    "import dask\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "import re\n",
    "import dateutil.parser\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import ujson\n",
    "import dask.dataframe as dd\n",
    "import itertools\n",
    "from dask import delayed\n",
    "from bokeh.io import output_notebook\n",
    "import collections\n",
    "import glob\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = [20150615]\n",
    "thresholds = [1]\n",
    "tw_files='tests/resources/small_tw_file*.gz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = [20150615]\n",
    "thresholds = [10]\n",
    "tw_files=['/Users/houghton/Desktop/tw/posts_sample_20150615_162235_aa.gz',\n",
    "          '/Users/houghton/Desktop/tw/posts_sample_20150615_162235_aa.gz']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dates = [20150615]\n",
    "thresholds = [50]\n",
    "tw_files='/Users/houghton/Desktop/tw/posts_sample_201506*.gz'         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = [20150615]\n",
    "thresholds = [10]\n",
    "tw_files='/Users/houghton/Desktop/tw/posts_sample_20150615_162235_a*.gz'      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dates = [20150615, 20150616, 20150617, 20150618, 20150619, 20150620, \n",
    "         20150621, 20150622, 20150623, 20150624, 20150625]\n",
    "\n",
    "sw_file = \"homer/stopwords.pickle\"\n",
    "tw_stopwords = pickle.load(open(sw_file, \"rb\"))\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "matcher = re.compile(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*')\n",
    "\n",
    "languages = ['en']\n",
    "def fold_pairs(counter, json_string):\n",
    "    parsed_json = ujson.loads(json_string)\n",
    "    if ('lang' in parsed_json and\n",
    "            'text' in parsed_json and\n",
    "            parsed_json['lang'] in languages):\n",
    "        text = parsed_json['text'].lower()\n",
    "\n",
    "        # remove hyperlinks\n",
    "        text = matcher.sub('', text)\n",
    "\n",
    "        # tokenize, dropping punctuation\n",
    "        tokens = tokenizer.tokenize(text)  # this should release GIL - be ok for parallelization\n",
    "\n",
    "        # drop stopwords\n",
    "        tokens = filter(lambda x: x not in tw_stopwords, tokens)\n",
    "\n",
    "        date = int(dateutil.parser.parse(parsed_json['created_at']).strftime(\"%Y%m%d\"))\n",
    "        \n",
    "        if date not in counter:\n",
    "            counter[date] = collections.Counter()\n",
    "        for pair in combinations(tokens, 2):\n",
    "            counter[date][tuple(sorted(pair))] += 1\n",
    "\n",
    "    return counter\n",
    "\n",
    "def merge_folds(a, b):\n",
    "    if isinstance(a, dict):\n",
    "        series = {key:pd.Series(c) for key, c in a.items() if len(c)}\n",
    "        a = pd.concat(series.values(), keys=series.keys())\n",
    "        \n",
    "    if isinstance(b, dict):\n",
    "        series = {key:pd.Series(c) for key, c in b.items() if len(c)}\n",
    "        b = pd.concat(series.values(), keys=series.keys())\n",
    "        \n",
    "    return a.add(b, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = db.read_text(tw_files, compression='gzip', collection=True)\n",
    "\n",
    "initial_counter = {date:collections.Counter() for date in dates}\n",
    "counters = lines.fold(binop=fold_pairs, combine=merge_folds, initial=initial_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[####################################### ] | 99% Completed |  2hr 27min 43.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-43:\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-47:\n",
      "Process ForkPoolWorker-42:\n",
      "Process ForkPoolWorker-44:\n",
      "Process ForkPoolWorker-45:\n",
      "Process ForkPoolWorker-46:\n",
      "Process ForkPoolWorker-41:\n",
      "Process ForkPoolWorker-48:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/houghton/anaconda/envs/py3k/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-71a830891edb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mProgressBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcounts_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnpartitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcounts_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'W2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcounts_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcounts_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/houghton/anaconda/envs/py3k/lib/python3.5/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mExtra\u001b[0m \u001b[0mkeywords\u001b[0m \u001b[0mto\u001b[0m \u001b[0mforward\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mget\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \"\"\"\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/houghton/anaconda/envs/py3k/lib/python3.5/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mdsk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdask\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0mresults_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/houghton/anaconda/envs/py3k/lib/python3.5/site-packages/dask/multiprocessing.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(dsk, keys, num_workers, func_loads, func_dumps, optimize_graph, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m         result = get_async(pool.apply_async, len(pool._pool), dsk3, keys,\n\u001b[1;32m     85\u001b[0m                            \u001b[0mget_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_process_get_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                            dumps=dumps, loads=loads, **kwargs)\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/houghton/anaconda/envs/py3k/lib/python3.5/site-packages/dask/async.py\u001b[0m in \u001b[0;36mget_async\u001b[0;34m(apply_async, num_workers, dsk, result, cache, get_id, raise_on_exception, rerun_exceptions_locally, callbacks, dumps, loads, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;31m# Main loop, wait on tasks to finish, insert new ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'waiting'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ready'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'running'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/houghton/anaconda/envs/py3k/lib/python3.5/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/houghton/anaconda/envs/py3k/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with ProgressBar():\n",
    "    counts = counters.compute()\n",
    "    counts_df = dd.from_pandas(counts.reset_index(level=[1,2]), npartitions=10)\n",
    "    counts_df.columns = ['W1', 'W2', 'Count']\n",
    "    counts_df = counts_df[counts_df['Count'] > thresholds[0]]\n",
    "\n",
    "    def name(i):\n",
    "        return str(counts_df.divisions[i]) + '_' + str(counts_df.divisions[i+1])\n",
    "\n",
    "    counts_df.to_csv('/Users/houghton/Desktop/tw/charleston/wel_*.csv.gz', \n",
    "                     name_function = name,\n",
    "                     compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with Profiler() as prof, ResourceProfiler(dt=5) as rprof, CacheProfiler() as cprof:\n",
    "    counts = counters.compute()\n",
    "visualize([prof, rprof, cprof])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with Profiler() as prof, ResourceProfiler(dt=5) as rprof, CacheProfiler() as cprof:\n",
    "    counts = counters.compute()\n",
    "visualize([prof, rprof, cprof])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_df = dd.from_pandas(counts.reset_index(level=[1,2]), npartitions=10)\n",
    "#counts_df.to_hdf('/Users/houghton/Desktop/tw/charleston/test.hdf', '/clusters', dropna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts_df.columns = ['W1', 'W2', 'Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts_df.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    counts = counters.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold=50\n",
    "def find_clusters(date, counter, threshold):\n",
    "    series = pd.Series(counter)\n",
    "    over_thresh = series[series>threshold]\n",
    "    over_thresh.to_hdf('/Users/houghton/Desktop/tw/charleston/test_%i_%i.hdf' %(date, threshold), '/clusters', dropna=True)\n",
    "\n",
    "    #clusters = homer.clusterer.find_clusters(pd.DataFrame(pairs))\n",
    "    #clusters.to_hdf('/Users/houghton/Desktop/tw/charleston/test_%i_%i.hdf' %(date, threshold), '/clusters', dropna=True)\n",
    "    \n",
    "lazy_find_clusters = delayed(find_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    collector = []\n",
    "    for date, counter in counters.compute().items():\n",
    "        collector.append(lazy_find_clusters(date, counter, threshold))\n",
    "    dask.compute(*collector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tw_files='tests/resources/testfile.txt'\n",
    "# tw_files=['/Users/houghton/Desktop/tw/posts_sample_20150615_162235_aa.gz',\n",
    "#            '/Users/houghton/Desktop/tw/posts_sample_20150615_162235_aa.gz']\n",
    "\n",
    "# tw_files='/Users/houghton/Desktop/tw/posts_sample_20150615_162235_a*.gz'\n",
    "\n",
    "tw_files='/Users/houghton/Desktop/tw/posts_sample_201506*.gz'\n",
    "languages=['en']\n",
    "\n",
    "save_threshold = 10\n",
    "\n",
    "sw_file = \"homer/stopwords.pickle\"\n",
    "tw_stopwords = pickle.load(open(sw_file, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = homer.parser.get_weighted_edgelist(tw_files, languages=['en'], save_threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = df.groupby('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "clusters = groups.apply(homer.clusterer.find_clusters_for_any_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster = groups.apply(lambda x: homer.find_clusters_over_threshold(x, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# frequencies on daily, parse json separate\n",
    "dates = ['20160618']\n",
    "thresholds = [10, 15]\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def find_pairs(parsed_json):\n",
    "    text = parsed_json['text'].lower()\n",
    "\n",
    "    # remove hyperlinks\n",
    "    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "\n",
    "    # tokenize, dropping punctuation\n",
    "    tokens = tokenizer.tokenize(text)  # this should release GIL - be ok for parallelization\n",
    "\n",
    "    # drop stopwords\n",
    "    tokens = filter(lambda x: x not in tw_stopwords, tokens)\n",
    "\n",
    "    date = dateutil.parser.parse(parsed_json['created_at']).strftime(\"%Y%m%d\")\n",
    "\n",
    "    sets = [tuple([date] + sorted(pair)) for pair in combinations(tokens, 2)]\n",
    "    return sets\n",
    "\n",
    "messages = db.read_text(tw_files, compression='gzip').map(ujson.loads)\n",
    "\n",
    "def selector(msg):\n",
    "    return 'lang' in msg and 'text' in msg and msg['lang'] in languages\n",
    "\n",
    "selection = messages.filter(selector)\n",
    "sets = selection.map(find_pairs).concat()\n",
    "\n",
    "for date in dates:\n",
    "    frequencies_on_date = delayed(sets.filter(lambda x: x[0] == date).frequencies())\n",
    "    #for threshold in thresholds:\n",
    "    #    above_threshold = frequencies_on_date.filter(lambda x: x[1]>threshold)\n",
    "    #    pairs = above_threshold.pluck([0][1], [0][2])\n",
    "        \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# frequencies on daily, parse json integral\n",
    "dates = [20150615]\n",
    "thresholds = [10, 15]\n",
    "languages = ['en']\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "matcher = re.compile(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*')\n",
    "\n",
    "def find_pairs(json_string):\n",
    "    msg = ujson.loads(json_string)\n",
    "    if 'lang' in msg and 'text' in msg and msg['lang'] in languages:\n",
    "    \n",
    "        text = msg['text'].lower()\n",
    "\n",
    "        # remove hyperlinks\n",
    "        text = matcher.sub('', text)\n",
    "\n",
    "        # tokenize, dropping punctuation\n",
    "        tokens = tokenizer.tokenize(text)  # this should release GIL - be ok for parallelization\n",
    "\n",
    "        # drop stopwords\n",
    "        tokens = filter(lambda x: x not in tw_stopwords, tokens)\n",
    "\n",
    "        date = int(dateutil.parser.parse(msg['created_at']).strftime(\"%Y%m%d\"))\n",
    "\n",
    "        sets = [tuple([date] + sorted(pair)) for pair in combinations(tokens, 2)]\n",
    "        return sets\n",
    "    \n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "sets = db.read_text(tw_files, compression='gzip').map(find_pairs).concat()\n",
    "\n",
    "\n",
    "for date in dates:\n",
    "    sets_on_date = sets.filter(lambda x: x[0] == date)\n",
    "    frequencies_on_date = sets_on_date.frequencies()\n",
    "    for threshold in thresholds:\n",
    "        above_threshold = frequencies_on_date.filter(lambda x: x[1]>threshold)\n",
    "        pairs = above_threshold.pluck([0]).pluck([1, 2])\n",
    "        break\n",
    "    break\n",
    "\n",
    "#frequencies_on_date.take(3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# frequencies on whole set\n",
    "dates = [20150615]\n",
    "thresholds = [10, 15]\n",
    "languages = ['en']\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def find_pairs(json_string):\n",
    "    msg = ujson.loads(json_string)\n",
    "    if 'lang' in msg and 'text' in msg and msg['lang'] in languages:\n",
    "    \n",
    "        text = msg['text'].lower()\n",
    "\n",
    "        # remove hyperlinks\n",
    "        text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "\n",
    "        # tokenize, dropping punctuation\n",
    "        tokens = tokenizer.tokenize(text)  # this should release GIL - be ok for parallelization\n",
    "\n",
    "        # drop stopwords\n",
    "        tokens = filter(lambda x: x not in tw_stopwords, tokens)\n",
    "\n",
    "        date = int(dateutil.parser.parse(msg['created_at']).strftime(\"%Y%m%d\"))\n",
    "\n",
    "        sets = [tuple([date] + sorted(pair)) for pair in combinations(tokens, 2)]\n",
    "        return sets\n",
    "    \n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "sets = db.read_text(tw_files, compression='gzip').map(find_pairs).concat()\n",
    "frequencies = sets.frequencies()\n",
    "\n",
    "template = pd.DataFrame([{'Date': 20170121, 'W1': 'Toad', 'W2': u'Bug', 'Count': 21}],\n",
    "                                columns=['Date', 'W1', 'W2', 'Count'])\n",
    "\n",
    "collector = []\n",
    "for date, threshold in itertools.product(dates, thresholds):\n",
    "    selection = frequencies.filter(lambda x: x[0][0] == date and x[1]>threshold)\n",
    "    expand = frequencies.map(lambda x: (x[0][0], x[0][1], x[0][2], x[1]))\n",
    "    uw_df = expand.to_dataframe(template)\n",
    "    collector.append(delayed(homer.clusterer.find_clusters(uw_df[['W1', 'W2']]).assign(Threshold=threshold)))\n",
    "\n",
    "clusters = dd.from_delayed(collector, meta=template)\n",
    "clusters.to_hdf('/Users/houghton/Desktop/tw/charleston/test.hdf', '/clusters', dropna=True)\n",
    "\n",
    "visualize([prof, rprof, cprof])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# via dataframe\n",
    "#data goes into the 25th of the month...\n",
    "\n",
    "dates = [20150616, 20150617, 20150618, 20150619, 20150620, 20150621, 20150622]\n",
    "thresholds = [120]\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def find_pairs(json_string):\n",
    "    msg = ujson.loads(json_string)\n",
    "    if 'lang' in msg and 'text' in msg and msg['lang'] in languages:\n",
    "    \n",
    "        text = msg['text'].lower()\n",
    "\n",
    "        # remove hyperlinks\n",
    "        text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "\n",
    "        # tokenize, dropping punctuation\n",
    "        tokens = tokenizer.tokenize(text)  # this should release GIL - be ok for parallelization\n",
    "\n",
    "        # drop stopwords\n",
    "        tokens = filter(lambda x: x not in tw_stopwords, tokens)\n",
    "\n",
    "        date = int(dateutil.parser.parse(msg['created_at']).strftime(\"%Y%m%d\"))\n",
    "\n",
    "        sets = [tuple([date] + sorted(pair)) for pair in combinations(tokens, 2)]\n",
    "        return sets\n",
    "    \n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "sets = db.read_text(tw_files, compression='gzip').map(find_pairs).concat()\n",
    "frequencies = sets.frequencies()\n",
    "expand = frequencies.map(lambda x: (x[0][0], x[0][1], x[0][2], x[1]))\n",
    "\n",
    "template = pd.DataFrame([{'Date': 20170121, 'W1': 'Toad', 'W2': u'Bug', 'Count': 21}],\n",
    "                            columns=['Date', 'W1', 'W2', 'Count'])\n",
    "\n",
    "weighted_df = expand.to_dataframe(template)\n",
    "\n",
    "lazy_find_clusters = delayed(homer.clusterer.find_clusters)\n",
    "\n",
    "collector = []\n",
    "for date, threshold in itertools.product(dates, thresholds):\n",
    "    on_date = weighted_df[weighted_df['Date']==date]\n",
    "    unweighted_df = on_date[on_date['Count'] >= threshold][['W1', 'W2']]\n",
    "    collector.append(lazy_find_clusters(unweighted_df).assign(Threshold=threshold))\n",
    "\n",
    "\n",
    "template2 = pd.DataFrame(data=[{'Set':'Ho Bean', 'k':5, 'threshold': 10, 'Date':20170101}],\n",
    "                         columns=['Set', 'k', 'threshold', 'Date'])    \n",
    "\n",
    "clusters = dd.from_delayed(collector, meta=template2)\n",
    "#clusters.to_hdf('/Users/houghton/Desktop/tw/charleston/test.hdf', '/clusters', dropna=True)\n",
    "    \n",
    "#visualize([prof, rprof, cprof])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    clusters.to_hdf('/Users/houghton/Desktop/tw/charleston/full_set_t100.hdf', '/clusters', dropna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with Profiler() as prof, ResourceProfiler(dt=5) as rparof, CacheProfiler() as cprof:\n",
    "    clusters.to_hdf('/Users/houghton/Desktop/tw/charleston/full_set.hdf', '/clusters', dropna=True)\n",
    "visualize([prof, rprof, cprof])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dask.compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unweighted_df.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unweighted_df.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thresholds = weighted_edge_list['Count'].unique()\n",
    "clusters_collector = []\n",
    "for t in np.array(thresholds):\n",
    "    if t < min_threshold:\n",
    "        continue\n",
    "    cluster = delayed(find_clusters_over_threshold)(weighted_edge_list, t)\n",
    "    clusters_collector.append(cluster)\n",
    "\n",
    "clusters = dd.from_delayed(clusters_collector)\n",
    "return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "template = pd.DataFrame(data=[{'Set':'Ho Bean', 'k':5, 'threshold': 10, 'Date':20170101}],\n",
    "                        columns=['Set', 'k', 'threshold', 'Date'])\n",
    "\n",
    "collector = []\n",
    "for date in [20160618]:\n",
    "    selection = w_el[w_el.Date == date]\n",
    "    df = homer.clusterer.find_clusters_for_any_threshold(selection,\n",
    "                                         min_threshold=10)\n",
    "\n",
    "    df_2 = df.assign(Date=date)\n",
    "    collector.append(df_2)\n",
    "\n",
    "clusters = dd.from_delayed(collector, meta=template)        \n",
    "#clusters = dd.concat(collector)        \n",
    "#clusters = dd.concat(collector, interleave_partitions=True)\n",
    "#clusters = clusters.repartition(npartitions=1)  # Todo: This partitioning is problematic\n",
    "#clusters = clusters.reset_index(drop=True) #Todo: need to set an index which is unlikely to\n",
    "# also be a word in a cluster.\n",
    "#clusters.to_hdf(output_globstring, '/clusters', dropna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with Profiler() as prof, ResourceProfiler(dt=0.25) as rprof, CacheProfiler() as cprof:\n",
    "    groups.Count.mean().compute()\n",
    "visualize([prof, rprof, cprof]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with Profiler() as prof, ResourceProfiler(dt=0.25) as rprof, CacheProfiler() as cprof:\n",
    "    clusters.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with Profiler() as prof, ResourceProfiler(dt=0.25) as rprof, CacheProfiler() as cprof:\n",
    "    df.to_hdf('/Users/houghton/Desktop/tw/charleston/wel.hdf', key='/weighted_edge_list', dropna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.categorize('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.repartition(divisions=[0, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with Profiler() as prof, ResourceProfiler(dt=0.25) as rprof, CacheProfiler() as cprof:\n",
    "    df.compute()\n",
    "\n",
    "visualize([prof, rprof, cprof])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize([prof, rprof, cprof]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = homer.get_weighted_edgelist(tw_files, languages=['en'], save_threshold=10)\n",
    "df.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#with ProgressBar():\n",
    "with Profiler() as prof, ResourceProfiler(dt=0.25) as rprof, CacheProfiler() as cprof:\n",
    "    homer.build_weighted_edgelist_db(tw_files, \n",
    "                                     output_globstring='/Users/houghton/Desktop/tw/charleston/weighted_edgelists_*.hdf',\n",
    "                                     save_threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "template = pd.DataFrame([{'Date': 20170121, 'W1': 'Toad', 'W2': u'Bug', 'Count': 21}],\n",
    "                            columns=['Date', 'W1', 'W2', 'Count'])\n",
    "save_threshold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cooccurences = db.read_text(tw_files).map(homer.parser.get_message_cooccurences,\n",
    "                                          languages=['en']).concat()\n",
    "frequencies = cooccurences.frequencies()\n",
    "over_threshold = frequencies.filter(lambda x: x[1] >= save_threshold)\n",
    "expand = over_threshold.map(lambda x: (x[0][0], x[0][1], x[0][2], x[1]))\n",
    "df = expand.to_dataframe(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with Profiler() as prof, ResourceProfiler(dt=0.25) as rprof, CacheProfiler() as cprof:\n",
    "    df.compute()\n",
    "    \n",
    "visualize([prof, rprof, cprof])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dask.diagnostics import visualize\n",
    "visualize([prof, rprof, cprof])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "template = pd.DataFrame([{'Date':20170121, 'W1':'Toad', 'W2':u'Bug', 'Count':21}], \n",
    "                        columns=['Date', 'W1', 'W2', 'Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cooccurences = db.read_text(tw_files).map(homer.parser.get_message_cooccurences,\n",
    "                                          languages=languages).concat()\n",
    "\n",
    "#cooccurences.take(3)\n",
    "frequencies = cooccurences.frequencies()\n",
    "\n",
    "over_threshold = frequencies.filter(lambda x: x[1] >= keep_threshold)\n",
    "expand = over_threshold.map(lambda x: (x[0][0], x[0][1], x[0][2], x[1]))\n",
    "df = expand.to_dataframe(template)\n",
    "\n",
    "#frequencies = cooccurences.frequencies()\n",
    "# dicts = frequencies.map(\n",
    "#        lambda x: {'Date': int(x[0][0]),\n",
    "#                    'W1': str(x[0][1]),\n",
    "#                    'W2': str(x[0][2]),\n",
    "#                    'Count': int(x[1])})\n",
    "#     df = dicts.to_dataframe()\n",
    "\n",
    "\n",
    "with ProgressBar():\n",
    "#with Profiler() as prof, ResourceProfiler(dt=0.25) as rprof, CacheProfiler() as cprof:\n",
    "    #out = over_threshold.take(3)\n",
    "    #size = over_threshold.count().compute()\n",
    "    df.head()\n",
    "    \n",
    "#print(size)\n",
    "\n",
    "#out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cooccurences = db.read_text(tw_files).map(homer.parser.get_message_cooccurences,\n",
    "                                          languages=languages).concat()\n",
    "\n",
    "#cooccurences.take(3)\n",
    "frequencies = cooccurences.frequencies()\n",
    "\n",
    "over_threshold = frequencies.filter(lambda x: x[1] >= keep_threshold)\n",
    "expand = over_threshold.map(lambda x: (*x[0], x[1]))\n",
    "df = expand.to_dataframe(template)\n",
    "\n",
    "#frequencies = cooccurences.frequencies()\n",
    "# dicts = frequencies.map(\n",
    "#        lambda x: {'Date': int(x[0][0]),\n",
    "#                    'W1': str(x[0][1]),\n",
    "#                    'W2': str(x[0][2]),\n",
    "#                    'Count': int(x[1])})\n",
    "#     df = dicts.to_dataframe()\n",
    "\n",
    "\n",
    "with ProgressBar():\n",
    "#with Profiler() as prof, ResourceProfiler(dt=0.25) as rprof, CacheProfiler() as cprof:\n",
    "    #out = over_threshold.take(3)\n",
    "    #size = over_threshold.count().compute()\n",
    "    df.head()\n",
    "    \n",
    "#print(size)\n",
    "\n",
    "#out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = over_threshold.to_dataframe(columns=[['Date', 'W1', 'W2'], 'Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dask.diagnostics import visualize\n",
    "visualize([prof, rprof, cprof])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "over_threshold.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cooccurences = db.read_text(tw_files).map(homer.parser.get_message_cooccurences,\n",
    "                                          languages=languages).concat()\n",
    "\n",
    "#cooccurences.take(3)\n",
    "frequencies = cooccurences.frequencies()\n",
    "\n",
    "#over_threshold = frequencies.filter(lambda x: x[1] > keep_threshold)\n",
    "frequencies.remove(lambda x: x[1] < keep_threshold)\n",
    "\n",
    "#frequencies = cooccurences.frequencies()\n",
    "# dicts = frequencies.map(\n",
    "#        lambda x: {'Date': int(x[0][0]),\n",
    "#                    'W1': str(x[0][1]),\n",
    "#                    'W2': str(x[0][2]),\n",
    "#                    'Count': int(x[1])})\n",
    "#     df = dicts.to_dataframe()\n",
    "\n",
    "with ProgressBar():\n",
    "    out = frequencies.take(3)\n",
    "    #size = frequencies.count().compute()\n",
    "    \n",
    "print(size)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "over_threshold.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = homer.parser.get_weighted_edgelist(tw_files='tests/resources/testfile.txt',\n",
    "                                        languages=['en'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collection = homer.Homer(weighted_edge_list_globstring='tests/resources/Making_Connections_Generated_Data.hdf')                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection.compute_clusters('working/MC_gen_clusters_*.hdf', min_threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection.compute_relations('working/MC_gen_relations_*.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection.compute_transition_clusters('working/MC_gen_transitions_*.hdf', min_threshold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection.transition_clusters.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collection = homer.Homer(clusters_globstring='working/MC_gen_clusters_*.hdf', \n",
    "                         transition_clusters_globstring='working/MC_gen_transitions_*.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection.compute_transition_list('working/MC_gen_transitions_list.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection.compute_tree(tree_filename='working/MC_gen_tree.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection = homer.Homer(weighted_edge_list_globstring='tests/resources/Making_Connections_Generated_Data.hdf',\n",
    "                         clusters_globstring='working/MC_gen_clusters_*.hdf', \n",
    "                         #relations_globstring='working/MC_gen_relations_*.hdf',\n",
    "                         transition_clusters_globstring='working/MC_gen_transitions_*.hdf',\n",
    "                         transitions_filename='working/MC_gen_transitions_list.pickle',\n",
    "                         #tree_filename='working/MC_gen_tree.json.gz'\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = glob.glob('../../../mnt/nfs-marketdepth/houghton/data/twitter/raw/posts_sample_20150618*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pick up from saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection = homer.Homer(tree_filename='working/MC_gen_tree.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree = collection.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "homer.tree.draw_series(node, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "homer.tree.draw_series(tree.find(\"12\"),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection.clusters.reset_index(drop=True).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = collection.clusters.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = collection.clusters[collection.clusters['k']==7].compute()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "child = root.find(str(a.index[0]))\n",
    "child.k_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = collection.clusters[collection.clusters['k']==6][collection.clusters['Date']==1].compute()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = collection.clusters[collection.clusters['k']==5][collection.clusters['Date']==1].compute()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parent = root.find('360431357983146192')\n",
    "parent.k_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for n in homer.tree.walk_k_ancestry(parent):\n",
    "    print(n, n.k_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(zip((1,2), (11, 22), (111, 222)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,20))\n",
    "ax = plt.gca()\n",
    "ax_test = ax.twinx()\n",
    "parent.layout(ax_test)\n",
    "parent.set_bottom(100)\n",
    "parent.set_center(100)\n",
    "parent.draw(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parent.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.k_children[2] is b.k_children[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection.clusters[collection.clusters['k']==7].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection.clusters[(collection.clusters['k']==6)].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py3k]",
   "language": "python",
   "name": "Python [py3k]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
